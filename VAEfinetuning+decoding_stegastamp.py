# -*- coding: utf-8 -*-
"""Finetuning+decoding_StegaStamp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZOdd3jCgBBuL2Fu9OZ1CdJyx-_fsjNH9
"""

from google.colab import drive
drive.mount('/content/drive')

DATA_TAR_FILE = "/content/drive/MyDrive/Invisible_watermark_removal_data_models/400x400_paired_data.tar.gz"
SAVE_DIR = "/content/vae_finetuning"
!mkdir -p "$SAVE_DIR"

!cp "$DATA_TAR_FILE" "/content/"

!tar -xzf /content/400x400_paired_data.tar.gz

DATA_ROOT = "/content/space/mcdonald-syn01/1/projects/jsawant/DSC261/StegaStamp/400x400_paired_data"

!pip install -q diffusers==0.30.0 transformers accelerate torch torchvision tqdm pillow termcolor

import os
from PIL import Image
from tqdm import tqdm
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from torchvision.utils import save_image
from diffusers import AutoencoderKL
from termcolor import colored

# config
EPOCHS = 10
BATCH_SIZE = 8              # lowered for diagnostics (paper uses 16)
LR = 1e-5
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
GRAD_CLIP = 1.0

class WatermarkDataset(Dataset):
    def __init__(self, root, transform=None):
        self.root = root
        self.tf = transform
        all_folders = sorted([d for d in os.listdir(root) if not d.startswith(".")])
        self.pairs = []
        for folder in all_folders:
            fpath = os.path.join(root, folder)
            if not os.path.isdir(fpath):
                continue
            xw = os.path.join(fpath, f"{folder}_hidden.png")
            xi = os.path.join(fpath, f"{folder}_hidden_inv.png")
            if os.path.exists(xw) and os.path.exists(xi):
                self.pairs.append((xw, xi))
        print(f"Loaded {len(self.pairs)} paired samples from {len(all_folders)} folders.")

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        xw_path, xi_path = self.pairs[idx]
        xw = Image.open(xw_path).convert("RGB")
        xi = Image.open(xi_path).convert("RGB")
        if self.tf:
            xw = self.tf(xw)
            xi = self.tf(xi)
        return xw, xi

# --- Transform ---
transform = transforms.Compose([
    transforms.Resize((400, 400)),
    transforms.ToTensor(),   # range [0, 1]
])

dataset = WatermarkDataset(DATA_ROOT, transform)
loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)

vae = AutoencoderKL.from_pretrained(
    "stabilityai/sdxl-vae",
    # torch_dtype=torch.bfloat16  # prefer bf16 on A100; use fp16 only if you must
    torch_dtype=torch.float32
).to(DEVICE)
vae.enable_slicing()
vae.enable_tiling()
vae = vae.to(memory_format=torch.channels_last)
vae.train()
optimizer = torch.optim.Adam(vae.parameters(), lr=LR)
criterion = nn.MSELoss()

# Finetuning Loop (Algorithm 1 + diagnostics)

for epoch in range(EPOCHS):
    vae.train()
    total_loss = 0.0

    for step, (xw, xi) in enumerate(tqdm(loader, desc=f"Epoch {epoch+1}/{EPOCHS}"), start=1):
        xw, xi = xw.to(DEVICE, non_blocking=True), xi.to(DEVICE, non_blocking=True)
        optimizer.zero_grad(set_to_none=True)

        xw = xw.to(memory_format=torch.channels_last)
        xi = xi.to(memory_format=torch.channels_last)

        # # Diffusers VAEs expect inputs in [-1, 1]
        xw = xw * 2 - 1
        xi = xi * 2 - 1

        # --- Encode + Decode ---
        # with torch.autocast(device_type="cuda", dtype=torch.bfloat16):
        posterior = vae.encode(xw).latent_dist
        z = posterior.mean
        x_inv_hat = vae.decode(z).sample
        loss = criterion(x_inv_hat, xi)

        if torch.isnan(loss):
            print(colored("NaN loss detected – skipping batch", "red"))
            continue

        loss.backward()
        torch.nn.utils.clip_grad_norm_(vae.parameters(), GRAD_CLIP)
        optimizer.step()

        total_loss += loss.item() * xw.size(0)

        if step % 100 == 0:
            print(f"[Epoch {epoch+1} | Step {step}] Loss = {loss.item():.6f}")

    avg_loss = total_loss / len(dataset)
    print(colored(f"Epoch [{epoch+1}/{EPOCHS}] - Avg MSE Loss: {avg_loss:.6f}", "cyan"))

    # --- Save sample reconstructions ---
    with torch.no_grad():
        xw_vis, xi_vis = dataset[0]
        xw_vis = xw_vis.unsqueeze(0).to(DEVICE)
        xi_vis = xi_vis.unsqueeze(0).to(DEVICE)

        # channels-last to match model (optional)
        xw_vis = xw_vis.to(memory_format=torch.channels_last)
        xi_vis = xi_vis.to(memory_format=torch.channels_last)

        # # scale to [-1,1] for VAE
        xw_vis = xw_vis * 2 - 1
        xi_vis = xi_vis * 2 - 1

        # with torch.autocast(device_type="cuda", dtype=torch.bfloat16):
        posterior_vis = vae.encode(xw_vis)
        z_vis = posterior_vis.latent_dist.sample()
        x_hat_vis = vae.decode(z_vis).sample

    def denorm(x):
      x = (x + 1) / 2
      return x.clamp(0, 1)
    save_image(torch.cat([
      denorm(xw_vis),
      denorm(x_hat_vis),
      denorm(xi_vis)
    ], dim=0),
    os.path.join(SAVE_DIR, f"epoch{epoch+1}_recon.png"), nrow=3)
    print(f"Saved reconstruction preview → epoch{epoch+1}_recon.png")

# Make sure all model parameters are contiguous before saving
for param in vae.parameters():
    if param.data.is_contiguous():
        continue
    param.data = param.data.contiguous()

vae.save_pretrained(SAVE_DIR)
print(colored(f"Finetuning complete! Model saved to {SAVE_DIR}", "green"))

# -*- coding: utf-8 -*-
"""
VAE Evaluation with StegaStamp Decoding
Evaluates watermark removal by decoding 100-bit messages
"""

import os
import numpy as np
from PIL import Image, ImageOps
from tqdm import tqdm
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from diffusers import AutoencoderKL
import tensorflow as tf
import pandas as pd
from termcolor import colored

# Suppress TF warnings
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
tf.compat.v1.disable_eager_execution()

DATA_ROOT = "/content/drive/MyDrive/400x400_paired_data"
FINETUNED_VAE_DIR = "/content/drive/MyDrive/vae_finetuning/models_new_improved"  # Your finetuned VAE
TF_MODEL_DIR = "/content/drive/MyDrive/stegastamp_pretrained"  # StegaStamp decoder
VAE_OUT_DIR = "/content/drive/MyDrive/vae_finetuning/vae_output_nov29"  # Where to save VAE outputs
EVAL_CSV = "/content/drive/MyDrive/vae_finetuning/vae_eval_results_29.csv"

os.makedirs(VAE_OUT_DIR, exist_ok=True)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
BATCH_SIZE = 16
DTYPE = torch.float32  # Match your training dtype


# ============================================================================
# DATASET FOR INFERENCE
# ============================================================================
class InferenceDataset(Dataset):
    def __init__(self, root, transform=None):
        self.root = root
        self.tf = transform
        all_folders = sorted([d for d in os.listdir(root) if not d.startswith(".")])
        self.items = []

        for folder in all_folders:
            fpath = os.path.join(root, folder)
            if not os.path.isdir(fpath):
                continue

            xw_path = os.path.join(fpath, f"{folder}_hidden.png")
            secret_path = os.path.join(fpath, f"{folder}_secret.txt")

            if os.path.exists(xw_path) and os.path.exists(secret_path):
                self.items.append({
                    "id": folder,
                    "hidden": xw_path,
                    "secret": secret_path
                })

        print(f"Loaded {len(self.items)} items for evaluation.")

    def __len__(self):
        return len(self.items)

    def __getitem__(self, idx):
        item = self.items[idx]
        xw = Image.open(item["hidden"]).convert("RGB")
        if self.tf:
            xw = self.tf(xw)
        return xw, item["id"]

    def get_metadata(self):
        return self.items


# ============================================================================
# LOAD FINETUNED VAE
# ============================================================================
print(colored("Loading finetuned VAE...", "yellow"))
vae = AutoencoderKL.from_pretrained(
    FINETUNED_VAE_DIR,
    torch_dtype=DTYPE
).to(DEVICE)
vae.enable_slicing()
vae.enable_tiling()
vae = vae.to(memory_format=torch.channels_last)
vae.eval()
print(colored("✅ VAE loaded successfully", "green"))


# ============================================================================
# VAE RECONSTRUCTION INFERENCE
# ============================================================================
transform = transforms.Compose([
    transforms.Resize((400, 400)),
    transforms.ToTensor(),  # [0, 1]
])

dataset = InferenceDataset(DATA_ROOT, transform)
loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False,
                   num_workers=4, pin_memory=True)
metadata = dataset.get_metadata()

def denorm(x):
    """Convert [-1, 1] to [0, 1]"""
    return ((x + 1) / 2).clamp(0, 1)

print(colored("\nRunning VAE reconstruction inference...", "cyan"))

with torch.no_grad():
    for xw_batch, ids_batch in tqdm(loader, desc="VAE Reconstruction"):
        xw_batch = xw_batch.to(DEVICE, non_blocking=True)
        xw_batch = xw_batch.to(memory_format=torch.channels_last)

        # Scale to [-1, 1] (matching your training)
        xw_batch = xw_batch * 2 - 1

        # Encode and decode
        posterior = vae.encode(xw_batch).latent_dist
        z = posterior.mean  # Use mean for deterministic reconstruction
        x_recon = vae.decode(z).sample

        # Denormalize to [0, 1]
        x_recon = denorm(x_recon)

        # Save reconstructed images
        for i in range(x_recon.size(0)):
            img_tensor = x_recon[i].cpu().float()
            img_id = ids_batch[i]
            save_path = os.path.join(VAE_OUT_DIR, f"{img_id}_vae.png")
            img_pil = transforms.ToPILImage()(img_tensor)
            img_pil.save(save_path, 'PNG')

print(colored(f"✅ VAE reconstruction complete. Saved to {VAE_OUT_DIR}", "green"))


# ============================================================================
# STEGASTAMP TENSORFLOW DECODER
# ============================================================================
def load_decoder_handles(model_dir):
    """Load StegaStamp TensorFlow decoder model"""
    g = tf.Graph()
    sess = tf.compat.v1.Session(graph=g)

    meta = tf.compat.v1.saved_model.loader.load(
        sess, [tf.saved_model.SERVING], model_dir
    )
    sig = meta.signature_def["serving_default"]

    # Get tensor names from signature
    inp_image_name = sig.inputs["image"].name
    inp_secret_name = sig.inputs["secret"].name
    out_bits_name = sig.outputs["decoded"].name

    # Get tensor handles
    inp_image = g.get_tensor_by_name(inp_image_name)
    inp_secret = g.get_tensor_by_name(inp_secret_name)
    out_bits = g.get_tensor_by_name(out_bits_name)

    return sess, {"image": inp_image, "secret": inp_secret}, out_bits


def preprocess_image_for_decoder(path):
    """Preprocess image for StegaStamp decoder: 400x400, RGB, [0,1]"""
    img = Image.open(path).convert("RGB")
    img = ImageOps.fit(img, (400, 400))
    arr = np.asarray(img).astype(np.float32) / 255.0
    return arr


def decode_bits(sess, inputs_dict, out_tensor, img_path):
    """Decode 100-bit message from image using StegaStamp decoder"""
    if not os.path.isfile(img_path):
        return None

    # Preprocess image
    img_arr = preprocess_image_for_decoder(img_path)

    # Dummy secret (required by decoder interface, but not used)
    dummy_secret = np.zeros((1, 100), dtype=np.float32)

    # Feed to decoder: image shape must be (1, 400, 400, 3)
    feed_dict = {
        inputs_dict["image"]: img_arr[np.newaxis, ...],
        inputs_dict["secret"]: dummy_secret
    }

    # Run decoder
    pred_bits = sess.run(out_tensor, feed_dict=feed_dict)
    pred_bits = np.squeeze(pred_bits)

    return pred_bits.astype(np.int64)


def read_secret_bits(txt_path):
    """Read 100-bit ground truth secret from text file"""
    with open(txt_path, "r") as f:
        secret_str = f.read().strip()

    # Extract only binary characters
    secret_str = "".join(ch for ch in secret_str if ch in "01")

    # Convert to numpy array
    bits = np.fromiter((1 if c == '1' else 0 for c in secret_str), dtype=np.int64)

    assert bits.size == 100, f"Expected 100 bits, got {bits.size} from {txt_path}"
    return bits


def compute_hamming(pred_bits, true_bits):
    """Compute Hamming distance and accuracy"""
    if pred_bits is None or true_bits is None:
        return None, None, 100

    L = min(len(pred_bits), len(true_bits))
    hamming_dist = int(np.sum(pred_bits[:L] != true_bits[:L]))
    accuracy = 1.0 - (hamming_dist / L)

    return hamming_dist, accuracy, L


# ============================================================================
# LOAD STEGASTAMP DECODER
# ============================================================================
print(colored("\nLoading StegaStamp decoder...", "yellow"))
try:
    sess, inputs_dict, out_bits = load_decoder_handles(TF_MODEL_DIR)
    print(colored(f" StegaStamp decoder loaded from {TF_MODEL_DIR}", "green"))
except Exception as e:
    print(colored(f"Error loading decoder: {e}", "red"))
    exit(1)


# ============================================================================
# EVALUATION LOOP: DECODE AND COMPUTE METRICS
# ============================================================================
print(colored("\nStarting evaluation (decoding and Hamming distance)...", "cyan"))

records = []
vae_files_found = 0

for item in tqdm(metadata, desc="Decoding & Evaluation"):
    folder_id = item["id"]
    hidden_path = item["hidden"]
    secret_path = item["secret"]

    # Read ground truth secret
    true_bits = read_secret_bits(secret_path)

    # -------------------------
    # 1. Decode ORIGINAL watermarked image (baseline)
    # -------------------------
    pred_bits_orig = decode_bits(sess, inputs_dict, out_bits, hidden_path)
    ham_orig, acc_orig, L_orig = compute_hamming(pred_bits_orig, true_bits)

    # -------------------------
    # 2. Decode VAE RECONSTRUCTED image (test)
    # -------------------------
    vae_path = os.path.join(VAE_OUT_DIR, f"{folder_id}_vae.png")
    pred_bits_vae = decode_bits(sess, inputs_dict, out_bits, vae_path)

    ham_vae, acc_vae, L_vae = None, None, None
    if pred_bits_vae is not None:
        vae_files_found += 1
        ham_vae, acc_vae, L_vae = compute_hamming(pred_bits_vae, true_bits)

    # Store results
    records.append({
        "id": folder_id,
        "hamming_original": ham_orig,
        "accuracy_original": acc_orig,
        "hamming_vae": ham_vae,
        "accuracy_vae": acc_vae
    })

df = pd.DataFrame(records).sort_values("id")
df.to_csv(EVAL_CSV, index=False)

print("\n" + "="*70)
print("EVALUATION COMPLETE")
print("="*70)
print(f"Results saved to: {EVAL_CSV}")
print(f"Total images evaluated: {len(records)}")
print(f"VAE reconstructions found: {vae_files_found}/{len(records)}")

print("\n" + "="*70)
print("SUMMARY STATISTICS")
print("="*70)
print(f"Original Watermarked Images:")
print(f"  Mean Accuracy: {df['accuracy_original'].mean():.4f} ({df['accuracy_original'].mean()*100:.2f}%)")
print(f"  Mean Hamming Distance: {df['hamming_original'].mean():.2f}/100 bits")

print(f"\nVAE Reconstructed Images:")
vae_acc_mean = df['accuracy_vae'].dropna().mean()
vae_ham_mean = df['hamming_vae'].dropna().mean()
print(f"  Mean Accuracy: {vae_acc_mean:.4f} ({vae_acc_mean*100:.2f}%)")
print(f"  Mean Hamming Distance: {vae_ham_mean:.2f}/100 bits")

print("\n" + "="*70)
print("INTERPRETATION")
print("="*70)
print("Expected results for successful watermark removal:")
print("  • Original: ~95-100% accuracy (watermark intact)")
print("  • VAE Reconstructed: ~0-20% accuracy (watermark removed, random guessing)")
print("\nIf VAE accuracy is still high (>50%), watermark removal was unsuccessful.")
print("="*70)

print("\nFirst 10 results:")
print(df.head(10).to_string(index=False))

detected_orig = (df['accuracy_original'] > 0.5).sum()
detected_vae = (df['accuracy_vae'].dropna() > 0.5).sum()

print(f"\n" + "="*70)
print(f"DETECTION METRIC (Simplified)")
print(f"="*70)
print(f"Images with >50% bit accuracy (detected as watermarked):")
print(f"  Original: {detected_orig}/{len(df)} ({100*detected_orig/len(df):.1f}%)")
print(f"  VAE Reconstructed: {detected_vae}/{vae_files_found} ({100*detected_vae/vae_files_found:.1f}%)")
print(f"\nWatermark removal success: {100*(1 - detected_vae/vae_files_found):.1f}%")
print("="*70)



